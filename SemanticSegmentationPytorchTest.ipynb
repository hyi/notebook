{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from distutils.version import LooseVersion\n",
    "# Numerical libs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.io import loadmat\n",
    "import csv\n",
    "# Our libs\n",
    "from mit_semseg.dataset import TestDataset\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "from mit_semseg.utils import colorEncode, find_recursive, setup_logger\n",
    "from mit_semseg.lib.nn import user_scattered_collate, async_copy_to\n",
    "from mit_semseg.lib.utils import as_numpy\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from mit_semseg.config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120 120 120]\n",
      " [180 120 120]\n",
      " [  6 230 230]\n",
      " [ 80  50  50]\n",
      " [  4 200   3]\n",
      " [120 120  80]\n",
      " [140 140 140]\n",
      " [204   5 255]\n",
      " [230 230 230]\n",
      " [  4 250   7]\n",
      " [224   5 255]\n",
      " [235 255   7]\n",
      " [150   5  61]\n",
      " [120 120  70]\n",
      " [  8 255  51]\n",
      " [255   6  82]\n",
      " [143 255 140]\n",
      " [204 255   4]\n",
      " [255  51   7]\n",
      " [204  70   3]\n",
      " [  0 102 200]\n",
      " [ 61 230 250]\n",
      " [255   6  51]\n",
      " [ 11 102 255]\n",
      " [255   7  71]\n",
      " [255   9 224]\n",
      " [  9   7 230]\n",
      " [220 220 220]\n",
      " [255   9  92]\n",
      " [112   9 255]\n",
      " [  8 255 214]\n",
      " [  7 255 224]\n",
      " [255 184   6]\n",
      " [ 10 255  71]\n",
      " [255  41  10]\n",
      " [  7 255 255]\n",
      " [224 255   8]\n",
      " [102   8 255]\n",
      " [255  61   6]\n",
      " [255 194   7]\n",
      " [255 122   8]\n",
      " [  0 255  20]\n",
      " [255   8  41]\n",
      " [255   5 153]\n",
      " [  6  51 255]\n",
      " [235  12 255]\n",
      " [160 150  20]\n",
      " [  0 163 255]\n",
      " [140 140 140]\n",
      " [250  10  15]\n",
      " [ 20 255   0]\n",
      " [ 31 255   0]\n",
      " [255  31   0]\n",
      " [255 224   0]\n",
      " [153 255   0]\n",
      " [  0   0 255]\n",
      " [255  71   0]\n",
      " [  0 235 255]\n",
      " [  0 173 255]\n",
      " [ 31   0 255]\n",
      " [ 11 200 200]\n",
      " [255  82   0]\n",
      " [  0 255 245]\n",
      " [  0  61 255]\n",
      " [  0 255 112]\n",
      " [  0 255 133]\n",
      " [255   0   0]\n",
      " [255 163   0]\n",
      " [255 102   0]\n",
      " [194 255   0]\n",
      " [  0 143 255]\n",
      " [ 51 255   0]\n",
      " [  0  82 255]\n",
      " [  0 255  41]\n",
      " [  0 255 173]\n",
      " [ 10   0 255]\n",
      " [173 255   0]\n",
      " [  0 255 153]\n",
      " [255  92   0]\n",
      " [255   0 255]\n",
      " [255   0 245]\n",
      " [255   0 102]\n",
      " [255 173   0]\n",
      " [255   0  20]\n",
      " [255 184 184]\n",
      " [  0  31 255]\n",
      " [  0 255  61]\n",
      " [  0  71 255]\n",
      " [255   0 204]\n",
      " [  0 255 194]\n",
      " [  0 255  82]\n",
      " [  0  10 255]\n",
      " [  0 112 255]\n",
      " [ 51   0 255]\n",
      " [  0 194 255]\n",
      " [  0 122 255]\n",
      " [  0 255 163]\n",
      " [255 153   0]\n",
      " [  0 255  10]\n",
      " [255 112   0]\n",
      " [143 255   0]\n",
      " [ 82   0 255]\n",
      " [163 255   0]\n",
      " [255 235   0]\n",
      " [  8 184 170]\n",
      " [133   0 255]\n",
      " [  0 255  92]\n",
      " [184   0 255]\n",
      " [255   0  31]\n",
      " [  0 184 255]\n",
      " [  0 214 255]\n",
      " [255   0 112]\n",
      " [ 92 255   0]\n",
      " [  0 224 255]\n",
      " [112 224 255]\n",
      " [ 70 184 160]\n",
      " [163   0 255]\n",
      " [153   0 255]\n",
      " [ 71 255   0]\n",
      " [255   0 163]\n",
      " [255 204   0]\n",
      " [255   0 143]\n",
      " [  0 255 235]\n",
      " [133 255   0]\n",
      " [255   0 235]\n",
      " [245   0 255]\n",
      " [255   0 122]\n",
      " [255 245   0]\n",
      " [ 10 190 212]\n",
      " [214 255   0]\n",
      " [  0 204 255]\n",
      " [ 20   0 255]\n",
      " [255 255   0]\n",
      " [  0 153 255]\n",
      " [  0  41 255]\n",
      " [  0 255 204]\n",
      " [ 41   0 255]\n",
      " [ 41 255   0]\n",
      " [173   0 255]\n",
      " [  0 245 255]\n",
      " [ 71   0 255]\n",
      " [122   0 255]\n",
      " [  0 255 184]\n",
      " [  0  92 255]\n",
      " [184 255   0]\n",
      " [  0 133 255]\n",
      " [255 214   0]\n",
      " [ 25 194 194]\n",
      " [102 255   0]\n",
      " [ 92   0 255]]\n"
     ]
    }
   ],
   "source": [
    "colors = loadmat('../semantic-segmentation-pytorch/data/color150.mat')['colors']\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'wall', 2: 'building', 3: 'sky', 4: 'floor', 5: 'tree', 6: 'ceiling', 7: 'road', 8: 'bed', 9: 'windowpane', 10: 'grass', 11: 'cabinet', 12: 'sidewalk', 13: 'person', 14: 'earth', 15: 'door', 16: 'table', 17: 'mountain', 18: 'plant', 19: 'curtain', 20: 'chair', 21: 'car', 22: 'water', 23: 'painting', 24: 'sofa', 25: 'shelf', 26: 'house', 27: 'sea', 28: 'mirror', 29: 'rug', 30: 'field', 31: 'armchair', 32: 'seat', 33: 'fence', 34: 'desk', 35: 'rock', 36: 'wardrobe', 37: 'lamp', 38: 'bathtub', 39: 'railing', 40: 'cushion', 41: 'base', 42: 'box', 43: 'column', 44: 'signboard', 45: 'chest', 46: 'counter', 47: 'sand', 48: 'sink', 49: 'skyscraper', 50: 'fireplace', 51: 'refrigerator', 52: 'grandstand', 53: 'path', 54: 'stairs', 55: 'runway', 56: 'case', 57: 'pool', 58: 'pillow', 59: 'screen', 60: 'stairway', 61: 'river', 62: 'bridge', 63: 'bookcase', 64: 'blind', 65: 'coffee', 66: 'toilet', 67: 'flower', 68: 'book', 69: 'hill', 70: 'bench', 71: 'countertop', 72: 'stove', 73: 'palm', 74: 'kitchen', 75: 'computer', 76: 'swivel', 77: 'boat', 78: 'bar', 79: 'arcade', 80: 'hovel', 81: 'bus', 82: 'towel', 83: 'light', 84: 'truck', 85: 'tower', 86: 'chandelier', 87: 'awning', 88: 'streetlight', 89: 'booth', 90: 'television', 91: 'airplane', 92: 'dirt', 93: 'apparel', 94: 'pole', 95: 'land', 96: 'bannister', 97: 'escalator', 98: 'ottoman', 99: 'bottle', 100: 'buffet', 101: 'poster', 102: 'stage', 103: 'van', 104: 'ship', 105: 'fountain', 106: 'conveyer', 107: 'canopy', 108: 'washer', 109: 'plaything', 110: 'swimming', 111: 'stool', 112: 'barrel', 113: 'basket', 114: 'waterfall', 115: 'tent', 116: 'bag', 117: 'minibike', 118: 'cradle', 119: 'oven', 120: 'ball', 121: 'food', 122: 'step', 123: 'tank', 124: 'trade', 125: 'microwave', 126: 'pot', 127: 'animal', 128: 'bicycle', 129: 'lake', 130: 'dishwasher', 131: 'screen', 132: 'blanket', 133: 'sculpture', 134: 'hood', 135: 'sconce', 136: 'vase', 137: 'traffic', 138: 'tray', 139: 'ashcan', 140: 'fan', 141: 'pier', 142: 'crt', 143: 'plate', 144: 'monitor', 145: 'bulletin', 146: 'shower', 147: 'radiator', 148: 'glass', 149: 'clock', 150: 'flag'}\n"
     ]
    }
   ],
   "source": [
    "names = {}\n",
    "with open('../semantic-segmentation-pytorch/data/object150_info.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        # only take the first label among the synonyms of the labels\n",
    "        names[int(row[0])] = row[5].split(';')[0]\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(data, pred, cfg):\n",
    "    (img, info) = data\n",
    "\n",
    "    # print predictions in descending order\n",
    "    pred = np.int32(pred)\n",
    "    pixs = pred.size\n",
    "    uniques, counts = np.unique(pred, return_counts=True)\n",
    "    print(\"Predictions in [{}]:\".format(info))\n",
    "    for idx in np.argsort(counts)[::-1]:\n",
    "        name = names[uniques[idx] + 1]\n",
    "        ratio = counts[idx] / pixs * 100\n",
    "        if ratio > 0.1:\n",
    "            print(\"  {}: {:.2f}%\".format(name, ratio))\n",
    "\n",
    "    # colorize prediction\n",
    "    pred_color = colorEncode(pred, colors).astype(np.uint8)\n",
    "\n",
    "    # aggregate images and save\n",
    "    im_vis = np.concatenate((img, pred_color), axis=1)\n",
    "\n",
    "    img_name = info.split('/')[-1]\n",
    "    Image.fromarray(im_vis).save(\n",
    "        os.path.join(cfg.TEST.result, img_name.replace('.jpg', '.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(segmentation_module, loader, gpu):\n",
    "    segmentation_module.eval()\n",
    "\n",
    "    pbar = tqdm(total=len(loader))\n",
    "    for batch_data in loader:\n",
    "        # process data\n",
    "        batch_data = batch_data[0]\n",
    "        segSize = (batch_data['img_ori'].shape[0],\n",
    "                   batch_data['img_ori'].shape[1])\n",
    "        img_resized_list = batch_data['img_data']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = torch.zeros(1, cfg.DATASET.num_class, segSize[0], segSize[1])\n",
    "            scores = async_copy_to(scores, gpu)\n",
    "\n",
    "            for img in img_resized_list:\n",
    "                feed_dict = batch_data.copy()\n",
    "                feed_dict['img_data'] = img\n",
    "                del feed_dict['img_ori']\n",
    "                del feed_dict['info']\n",
    "                feed_dict = async_copy_to(feed_dict, gpu)\n",
    "\n",
    "                # forward pass\n",
    "                pred_tmp = segmentation_module(feed_dict, segSize=segSize)\n",
    "                scores = scores + pred_tmp / len(cfg.DATASET.imgSizes)\n",
    "\n",
    "            _, pred = torch.max(scores, dim=1)\n",
    "            pred = as_numpy(pred.squeeze(0).cpu())\n",
    "\n",
    "        # visualization\n",
    "        visualize_result(\n",
    "            (batch_data['img_ori'], batch_data['info']),\n",
    "            pred,\n",
    "            cfg\n",
    "        )\n",
    "\n",
    "        pbar.update(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert LooseVersion(torch.__version__) >= LooseVersion('0.4.0'), \\\n",
    "        'PyTorch>=0.4.0 is required'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET:\n",
      "  imgMaxSize: 1000\n",
      "  imgSizes: (300, 375, 450, 525, 600)\n",
      "  list_train: ../semantic-segmentation-pytorch/data/training.odgt\n",
      "  list_val: ../semantic-segmentation-pytorch/data/validation.odgt\n",
      "  num_class: 150\n",
      "  padding_constant: 8\n",
      "  random_flip: True\n",
      "  root_dataset: ../semantic-segmentation-pytorch/data/\n",
      "  segm_downsampling_rate: 8\n",
      "DIR: model\n",
      "MODEL:\n",
      "  arch_decoder: ppm_deepsup\n",
      "  arch_encoder: resnet50dilated\n",
      "  fc_dim: 2048\n",
      "  weights_decoder: \n",
      "  weights_encoder: \n",
      "TEST:\n",
      "  batch_size: 1\n",
      "  checkpoint: epoch_20.pth\n",
      "  result: result\n",
      "TRAIN:\n",
      "  batch_size_per_gpu: 2\n",
      "  beta1: 0.9\n",
      "  deep_sup_scale: 0.4\n",
      "  disp_iter: 20\n",
      "  epoch_iters: 5000\n",
      "  fix_bn: False\n",
      "  lr_decoder: 0.02\n",
      "  lr_encoder: 0.02\n",
      "  lr_pow: 0.9\n",
      "  num_epoch: 20\n",
      "  optim: SGD\n",
      "  seed: 304\n",
      "  start_epoch: 0\n",
      "  weight_decay: 0.0001\n",
      "  workers: 16\n",
      "VAL:\n",
      "  batch_size: 1\n",
      "  checkpoint: epoch_20.pth\n",
      "  visualize: False\n"
     ]
    }
   ],
   "source": [
    "cfg_file = \"config/ade20k-resnet50dilated-ppm_deepsup.yaml\"\n",
    "cfg.merge_from_file(cfg_file)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.arch_encoder = cfg.MODEL.arch_encoder.lower()\n",
    "cfg.MODEL.arch_decoder = cfg.MODEL.arch_decoder.lower()\n",
    "# absolute paths of model weights\n",
    "cfg.MODEL.weights_encoder = os.path.join(cfg.DIR, 'encoder_' + cfg.TEST.checkpoint)\n",
    "cfg.MODEL.weights_decoder = os.path.join(cfg.DIR, 'decoder_' + cfg.TEST.checkpoint)\n",
    "# checkpoint does not exist at this point\n",
    "#assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n",
    "#        os.path.exists(cfg.MODEL.weights_decoder), \"checkpoint does not exist!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single joined image of left, center, and right images to test for inference\n",
    "#img_names = [\"/projects/ncdot/2018/NC_2018/327/19/327001906055.jpg\", \"/projects/ncdot/2018/NC_2018/327/19/327001906051.jpg\", \"/projects/ncdot/2018/NC_2018/327/19/327001906052.jpg\"]\n",
    "#for idx in range(3):\n",
    "#    imgs[idx] = Image.open(img_names[idx])\n",
    "\n",
    "#dst = Image.new('RGB', (imgs[0].width+imgs[1].width+imgs[2].width, imgs[0].height))\n",
    "#dst.paste(imgs[0], (0,0))\n",
    "#dst.paste(imgs[1], (imgs[0].width,0))\n",
    "#dst.paste(imgs[2], (imgs[0].width+imgs[1].width,0))\n",
    "#dst.save('image/32700190605_512.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda gpu device count: 2\n",
      "cuda currently selected gpu device:  0\n"
     ]
    }
   ],
   "source": [
    "#imgs = [\"image/ADE_val_00001519.jpg\"]\n",
    "#imgs = [\"image/32700190605_512.jpg\"]\n",
    "imgs = [\"/projects/ncdot/2018/NC_2018/327/19/327001906052.jpg\"]\n",
    "#imgs = [\"/projects/ncdot/2018/NC_2018/757/83/757012315276.jpg\"]\n",
    "#imgs = [\"image/output0006.jpg\"]\n",
    "cfg.list_test = [{'fpath_img': x} for x in imgs]\n",
    "print(\"cuda gpu device count:\", torch.cuda.device_count())\n",
    "gpu = torch.cuda.current_device()\n",
    "print(\"cuda currently selected gpu device: \", gpu)\n",
    "# torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n",
      "Loading weights for net_decoder\n",
      "# samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions in [/projects/ncdot/2018/NC_2018/327/19/327001906052.jpg]:\n",
      "  sky: 29.10%\n",
      "  tree: 27.74%\n",
      "  grass: 21.61%\n",
      "  earth: 11.24%\n",
      "  fence: 5.24%\n",
      "  road: 2.54%\n",
      "  railing: 1.22%\n",
      "  building: 0.69%\n",
      "  path: 0.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:02<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference done!\n"
     ]
    }
   ],
   "source": [
    "# Network Builders\n",
    "net_encoder = ModelBuilder.build_encoder(\n",
    "    arch=cfg.MODEL.arch_encoder,\n",
    "    fc_dim=cfg.MODEL.fc_dim,\n",
    "    weights=cfg.MODEL.weights_encoder)\n",
    "net_decoder = ModelBuilder.build_decoder(\n",
    "    arch=cfg.MODEL.arch_decoder,\n",
    "    fc_dim=cfg.MODEL.fc_dim,\n",
    "    num_class=cfg.DATASET.num_class,\n",
    "    weights=cfg.MODEL.weights_decoder,\n",
    "    use_softmax=True)\n",
    "\n",
    "crit = nn.NLLLoss(ignore_index=-1)\n",
    "\n",
    "segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "\n",
    "# Dataset and Loader\n",
    "dataset_test = TestDataset(cfg.list_test, cfg.DATASET)\n",
    "loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=cfg.TEST.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=user_scattered_collate,\n",
    "    num_workers=5,\n",
    "    drop_last=True)\n",
    "\n",
    "segmentation_module.cuda()\n",
    "\n",
    "# Main loop\n",
    "test(segmentation_module, loader_test, gpu)\n",
    "\n",
    "print('Inference done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
